{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "!pip install datasets sentence-transformers faiss-cpu accelerate bitsandbytes gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import ast\n",
    "import re\n",
    "from typing import List\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We will be using a random dataset I found online that contains FAQ about a service called PetBacker. If you are finished with this notebook and like to experiment some more try out some other (small) datasets. \n",
    "\n",
    "You can find other datasets [here](https://huggingface.co/datasets?size_categories=or:%28size_categories:n%3C1K,size_categories:1K%3Cn%3C10K%29&sort=trending).\n",
    "\n",
    "#### Note\n",
    "I like to use the `cache_dir` parameter so that models and datasets gets saved in the working directory. This is useful if you want to later delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "dataset = load_dataset(\"JLK-ptbk/faq\", cache_dir=\"datasets\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "We need 3 models to solve this problem:\n",
    "- One to generate embeddings of the input sequences\n",
    "- A LLM to generate the output sequences\n",
    "- A tokenizer to convert the output sequences to the desired format\n",
    "\n",
    "You can find the other options for the LLM model [here](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?params=0%2C3&official=true) on the open LLM leaderboard.\n",
    "If you want to try out other embedding models have a look at [this leaderboard](https://huggingface.co/spaces/mteb/leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the models\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\", cache_folder=\"models\")\n",
    "\n",
    "# Load the LLM and its tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM-1.7B-Instruct\"\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=\"models\",\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True,\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you inspect the dataset you will see that it includes HTML and CSS tags. This is not useful for our RAG model, so we have to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': 0, 'index': 0, 'data': '[\\'How much does it cost to be a pet sitter?\\', \\'<p>Starting out as a pet sitter and offering your services on PetBacker is free, and you can set your own rates. When you get a reservation, a 15-25% service fee is deducted from the total booking amount.</p>\\\\r\\\\n<p>PetBacker service fees are reduced in levels based on the number of jobs you have completed. For more information do refer to the <a href=\"/help-center/pet-service-providers/pet-service-provider-incentive-program\">PetBacker incentive program</a> for the benefits you will receive.</p>\\\\r\\\\n<p>Any pet sitter/host suspected of soliciting payment outside of PetBacker will be suspended without notice. <br />Example: <br />1. Requesting customers to revise a booking from many days/visits/walks to one<br />2. Promoting other platforms<br />3. Requesting customers contact you outside by visiting Facebook, Google search, Map or other social media<br />4. Exchange of contacts or meet up before booking but not closing the deal (Penalty of 40% of the Fees to remove your account restriction)<br />5. Giving customers an unreasonable low rate or discount as a deposit (e.g. USD$1 for a Booking to get contact or to deal outside)<br />6. Cancelling or getting cancelled for a hired booking without good reason<br />7. Subsequent bookings or extensions not made via the platform<br />8. Misleading customers that PetBacker charges fees without informing them the importance of Pet Insurance that PetBacker includes in the hope they will deal with you outside<br />9. Requesting for Cash payment outside the platform<br />10. Requesting customers to pay Hidden fees outside the platform</p>\\\\r\\\\n<p>To restore your service fees from:<br />- First time suspected mistake, 30% to 20%, complete 5 new bookings<br />- Second time suspected mistake, 30% to 20%, complete 10 new bookings.</p>\\\\r\\\\n<p>For the third suspected mistake onwards, you will not receive anymore broadcast job requests, only normal Direct job requests at 35% of the fees.</p>\\\\r\\\\n<p>To fully restore your account you also have the option to join the <a href=\"/help-center/sponsors\">Sponsor Program</a> for a yearly fee.</p>\\\\r\\\\n<p>&nbsp;</p>\\\\r\\\\n<p><strong>You might also be interested in</strong></p>\\\\r\\\\n<p><a href=\"/help-center/sponsors\">Why be a sponsor?</a></p>\\\\r\\\\n<p><a href=\"/help-center/pet-service-providers/what-are-the-service-fees\">Why PetBacker need the service fees?</a>&nbsp;</p>\\\\r\\\\n<p><a href=\"/help-center/pet-service-providers/pet-service-provider-incentive-program\">Pet Service Provider Incentive Program</a></p>\\\\r\\\\n<p><a href=\"/help-center/policies/terms-of-use\">What are the terms of use?</a></p>\\']'}\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "We want the dataset to contain data in this format for each row:\n",
    "\n",
    "`\"Q: What methods of payment does PetBacker accept? A: The methods of payment vary by region...\"`\n",
    "\n",
    "You can choose to create a new column or adjust the existing column that contains the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags\n",
    "def remove_html_tags(text: str) -> str:\n",
    "    # TODO: Remove HTML tags from the text and return the cleaned text\n",
    "    return \n",
    "\n",
    "def apply_cleaning(row) -> dict:\n",
    "    # Get the data from the row\n",
    "    data = row[\"data\"]\n",
    "\n",
    "    # Remove HTML tags\n",
    "    data = remove_html_tags(data)\n",
    "\n",
    "    # TODO: Normalize the whitespace\n",
    "\n",
    "    # TODO: Deal with list formatting of the data\n",
    "\n",
    "    # TODO: Format the text to be like \"Q: ... A: ...\"\n",
    "    return {\"text\" : ...}\n",
    "\n",
    "# Delete all the unused columns (and the old data column)\n",
    "cleaned_dataset = dataset.map(apply_cleaning, remove_columns=[\"Unnamed: 0\", \"index\", \"data\"])\n",
    "\n",
    "# Inspect the claned dataset\n",
    "print(cleaned_dataset[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the embeddings\n",
    "Now that the dataset is clean we will generate the embeddings using the `embedding_model`\n",
    "\n",
    "This step is used to convert the data to vectors, so we can quickly lookup the most similar vectors to the input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate the embeddings for the cleaned dataset\n",
    "ds_with_embeddings = cleaned_dataset.map(lambda x : {\"embeddings\": ...})\n",
    "\n",
    "# TODO: Set the \"embeddings\" column as FAISS index\n",
    "# HINT: Have a look at the documentation: https://huggingface.co/docs/datasets/en/faiss_es\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving similar embeddings\n",
    "The next step is to retrieve the most similar embeddings to the input prompt. This is done in the following steps:\n",
    "- Convert the text to an embedding\n",
    "- Retrieve the most similar embeddings\n",
    "- Return the most similar text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get the right document for the prompt\n",
    "def search(prompt: str, k: int = 3) -> tuple:\n",
    "    # TODO: Embed the query\n",
    "    embedded_prompt = ...\n",
    "\n",
    "    # TODO: Search the dataset for the nearest examples\n",
    "    # HINT: Look at the documentation from the cell above\n",
    "    scores, retrieved_examples = ...\n",
    "\n",
    "    # Return the retrieved examples\n",
    "    return retrieved_examples[\"text\"]\n",
    "\n",
    "# Check if the search function works\n",
    "search(\"rebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a system prompt\n",
    "system_prompt = \"\"\"You are ...\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create more context\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Use a larger (quantized) model with bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Make a web interface for the model using gradio\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def generate_response(query):\n",
    "    return rag_pipeline(query, model, tokenizer, embedding_model, index, documents)\n",
    "\n",
    "\n",
    "gr.Interface(fn=generate_response, inputs=\"text\", outputs=\"text\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Harvest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
