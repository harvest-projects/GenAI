{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "!pip install datasets sentence-transformers faiss-cpu accelerate rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We will be using a random dataset I found online that contains FAQ about a service called PetBacker. If you are finished with this notebook and like to experiment some more try out some other (small) datasets. \n",
    "\n",
    "You can find other datasets [here](https://huggingface.co/datasets?size_categories=or:%28size_categories:n%3C1K,size_categories:1K%3Cn%3C10K%29&sort=trending).\n",
    "\n",
    "#### Note\n",
    "I like to use the `cache_dir` parameter so that models and datasets gets saved in the working directory. This is useful if you want to later delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "dataset = load_dataset(\"JLK-ptbk/faq\", cache_dir=\"datasets\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "We need 3 models to solve this problem:\n",
    "- One to generate embeddings of the input sequences (also know as the retriever)\n",
    "- A LLM to generate the output sequences\n",
    "- A tokenizer to convert the output sequences to the desired format\n",
    "\n",
    "You can find the other options for the LLM model [here](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?params=0%2C3&official=true) on the open LLM leaderboard.\n",
    "If you want to try out other embedding models have a look at [this leaderboard](https://huggingface.co/spaces/mteb/leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "# Download the models\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\", cache_folder=\"models\")\n",
    "\n",
    "# Load the LLM and its tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM-1.7B-Instruct\"\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=\"models\",\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True,\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"models\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llm,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the dataset\n",
    "If you inspect the dataset you will see that it includes HTML and CSS tags. This is not useful for our RAG model, so we have to remove them.\n",
    "\n",
    "It also seems to be a Python list saved a string, that consist of 2 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': 0, 'index': 0, 'data': '[\\'How much does it cost to be a pet sitter?\\', \\'<p>Starting out as a pet sitter and offering your services on PetBacker is free, and you can set your own rates. When you get a reservation, a 15-25% service fee is deducted from the total booking amount.</p>\\\\r\\\\n<p>PetBacker service fees are reduced in levels based on the number of jobs you have completed. For more information do refer to the <a href=\"/help-center/pet-service-providers/pet-service-provider-incentive-program\">PetBacker incentive program</a> for the benefits you will receive.</p>\\\\r\\\\n<p>Any pet sitter/host suspected of soliciting payment outside of PetBacker will be suspended without notice. <br />Example: <br />1. Requesting customers to revise a booking from many days/visits/walks to one<br />2. Promoting other platforms<br />3. Requesting customers contact you outside by visiting Facebook, Google search, Map or other social media<br />4. Exchange of contacts or meet up before booking but not closing the deal (Penalty of 40% of the Fees to remove your account restriction)<br />5. Giving customers an unreasonable low rate or discount as a deposit (e.g. USD$1 for a Booking to get contact or to deal outside)<br />6. Cancelling or getting cancelled for a hired booking without good reason<br />7. Subsequent bookings or extensions not made via the platform<br />8. Misleading customers that PetBacker charges fees without informing them the importance of Pet Insurance that PetBacker includes in the hope they will deal with you outside<br />9. Requesting for Cash payment outside the platform<br />10. Requesting customers to pay Hidden fees outside the platform</p>\\\\r\\\\n<p>To restore your service fees from:<br />- First time suspected mistake, 30% to 20%, complete 5 new bookings<br />- Second time suspected mistake, 30% to 20%, complete 10 new bookings.</p>\\\\r\\\\n<p>For the third suspected mistake onwards, you will not receive anymore broadcast job requests, only normal Direct job requests at 35% of the fees.</p>\\\\r\\\\n<p>To fully restore your account you also have the option to join the <a href=\"/help-center/sponsors\">Sponsor Program</a> for a yearly fee.</p>\\\\r\\\\n<p>&nbsp;</p>\\\\r\\\\n<p><strong>You might also be interested in</strong></p>\\\\r\\\\n<p><a href=\"/help-center/sponsors\">Why be a sponsor?</a></p>\\\\r\\\\n<p><a href=\"/help-center/pet-service-providers/what-are-the-service-fees\">Why PetBacker need the service fees?</a>&nbsp;</p>\\\\r\\\\n<p><a href=\"/help-center/pet-service-providers/pet-service-provider-incentive-program\">Pet Service Provider Incentive Program</a></p>\\\\r\\\\n<p><a href=\"/help-center/policies/terms-of-use\">What are the terms of use?</a></p>\\']'}\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "We want the dataset to contain data in this format for each row:\n",
    "\n",
    "`{'text': \"Q: What methods of payment does PetBacker accept? A: The methods of payment vary by region...\"}`\n",
    "\n",
    "You can choose to create a new column or adjust the existing column that contains the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Q: What methods of payment does PetBacker accept? A: The methods of payment vary by region. Some of the payment methods PetBacker accepts are direct bank in, and major credit cards, debit cards via Paypal.'}\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Feel free to import any libary for this step\n",
    "from bs4 import BeautifulSoup\n",
    "import ast\n",
    "\n",
    "# Remove HTML tags\n",
    "def remove_html_tags(text: str) -> str:\n",
    "    # TODO: Remove HTML tags from the text and return the cleaned text\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "def apply_cleaning(row) -> dict:\n",
    "    # Get the data from the row\n",
    "    data = row[\"data\"]\n",
    "\n",
    "    # TODO: Deal with list formatting of the data\n",
    "    data_list = ast.literal_eval(data)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    data_list = [remove_html_tags(item) for item in data_list]\n",
    "\n",
    "    # TODO: Normalize the whitespace\n",
    "    data_list = [\" \".join(item.split()) for item in data_list]\n",
    "\n",
    "    # TODO: Format the text to be like \"Q: ... A: ...\"\n",
    "    return {\"text\": f\"Q: {data_list[0]} A: {data_list[1]}\"}\n",
    "\n",
    "# Delete all the unused columns (and the old data column)\n",
    "cleaned_dataset = dataset.map(apply_cleaning, remove_columns=[\"Unnamed: 0\", \"index\", \"data\"])\n",
    "\n",
    "# Inspect the cleaned dataset\n",
    "print(cleaned_dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the embeddings\n",
    "Now that the dataset is clean we will generate the embeddings using the `embedding_model`. We will be using SentenceTransformers for this task. You can find the documentation [here](https://sbert.net/docs/sentence_transformer/usage/usage.html).\n",
    "\n",
    "This step is used to convert the data to vectors, so we can quickly lookup the most similar vectors to the input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b461cec39ce4804bdb5fef10336b25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'embeddings'],\n",
       "    num_rows: 182\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Generate the embeddings for the cleaned dataset\n",
    "ds_with_embeddings = cleaned_dataset.map(lambda x : {\"embeddings\": embedding_model.encode(x[\"text\"])})\n",
    "\n",
    "# TODO: Set the \"embeddings\" column as FAISS index\n",
    "# HINT: Have a look at the documentation: https://huggingface.co/docs/datasets/en/faiss_es\n",
    "ds_with_embeddings.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving similar embeddings\n",
    "The next step is to retrieve the most similar embeddings to the input prompt. This is done in the following steps:\n",
    "- Convert the text to an embedding\n",
    "- Retrieve the most similar embeddings\n",
    "- Return the most similar text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q: How do I rebook/extend booking with a sitter again? A: Follow the steps to rebook/extend your ongoing booking: 1) Navigate to the chat room with the booked sitter 3) Tap on \"Rebook/Extend Booking\" To rebook again your completed booking: 1) Navigate to the chat room with the booked sitter 3) Tap on \"Book Again\" 4) Fill in all required fields and submit your Request!',\n",
       " 'Q: What is a repeat customer? A: A repeat customer is a customer who has booked your services with payment through PetBacker and books your services again with payment through PetBacker a customer who extends an existing booking with payment through PetBacker You might be interested in What are the terms of use?',\n",
       " 'Q: How to redeem your payment? A: 1. Go to profile. 2. Tab on Payment Account. 3. Select the Earning History. 4. Then click on the redeem now button to redeem your money. You might be interested in What are the terms of use?']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a function to get the right document for the prompt\n",
    "def search(prompt: str, k: int = 3) -> tuple:\n",
    "    # TODO: Embed the query\n",
    "    embedded_prompt = embedding_model.encode(prompt)\n",
    "\n",
    "    # TODO: Search the dataset for the nearest examples\n",
    "    # HINT: Look at the documentation from the cell above\n",
    "    _, retrieved_examples = ds_with_embeddings.get_nearest_examples(\"embeddings\", embedded_prompt, k=k)\n",
    "\n",
    "    # Return the retrieved examples\n",
    "    return retrieved_examples[\"text\"]\n",
    "\n",
    "# Check if the search function works\n",
    "search(\"rebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the prompt\n",
    "With the function above we can lookup relevant information for a given prompt. The next step is to format the prompt in a way that the LLM can generate the correct output. We want to format the given prompt in a way like this:\n",
    "\n",
    "`\"Question: {User prompt} Context: {Retrieved text}\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: rebook \n",
      "Context: Follow the steps to rebook/extend your ongoing booking: 1) Navigate to the chat room with the booked sitter 3) Tap on \"Rebook/Extend Booking\" To rebook again your completed booking: 1) Navigate to the chat room with the booked sitter 3) Tap on \"Book Again\" 4) Fill in all required fields and submit your Request!\n",
      "A repeat customer is a customer who has booked your services with payment through PetBacker and books your services again with payment through PetBacker a customer who extends an existing booking with payment through PetBacker You might be interested in What are the terms of use?\n",
      "1. Go to profile. 2. Tab on Payment Account. 3. Select the Earning History. 4. Then click on the redeem now button to redeem your money. You might be interested in What are the terms of use?\n"
     ]
    }
   ],
   "source": [
    "def format_prompt(prompt: str, k:int) -> str:\n",
    "    \"\"\"using the retrieved documents we will prompt the model to generate our responses\"\"\"\n",
    "    # TODO: Use the search function to get the retrieved documents\n",
    "    retrieved_docs = search(prompt, k)\n",
    "\n",
    "    # TODO: remove the part of the retrieved documents that is not the answer\n",
    "    # HINT: So we only want to return the part that is after \"A: \"\n",
    "    retrieved_docs = [doc.split(\"A: \")[1] for doc in retrieved_docs]\n",
    "\n",
    "    # TODO: Join the retrieved documents into a single string\n",
    "    retrieved_docs = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "    # TODO: Add the retrieved documents to the prompt as context\n",
    "    # HINT: You can use the following format: \"Question: prompt \\n Context: retrieved docs\"\n",
    "    formatted_prompt = f\"Question: {prompt} \\nContext: {retrieved_docs}\"\n",
    "\n",
    "    return formatted_prompt\n",
    "\n",
    "print(format_prompt(\"rebook\", 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System prompt\n",
    "In the cell below you can specify your system prompt. This is the prompt that the LLM will use to generate the output. You can experiment with different prompts to see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a system prompt\n",
    "system_prompt = \"\"\"You are an assistant for answering questions.\n",
    "The user provides their question and your task is to give a fitting answer based on the given context.\n",
    "Your answer should be informative and concise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several payment methods that PetBacker accepts, including:\n",
      "\n",
      "1. Direct Bank in: This is a popular payment method that allows customers to pay for pet-related services directly from their bank account.\n",
      "2. Credit cards via PayPal: PetBacker also accepts credit card payments through PayPal, which is a secure payment gateway that protects both the customer and the merchant.\n",
      "3. Major credit cards: PetBacker accepts major credit cards such as Visa, Mastercard, and American Express.\n",
      "4. Debit cards via PayPal: Debit cards are also accepted, allowing customers to make payments directly from their bank account.\n",
      "5. PayPal: PetBacker accepts PayPal payments, which are processed through the PayPal platform.\n",
      "6. Checks: PetBacker accepts checks, which are processed through the PetBacker platform.\n",
      "\n",
      "The terms of use for PetBacker vary by region, and customers should ensure that they comply with the terms and conditions of use before making any payments.\n"
     ]
    }
   ],
   "source": [
    "# Prompt the model\n",
    "def prompt_model(prompt: str, k: int = 2):\n",
    "    formatted_prompt = format_prompt(prompt, k)\n",
    "\n",
    "    # Use the system prompt and the formatted prompt to generate the response\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "    ]\n",
    "\n",
    "    # The parameters for the LLM\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500, # Lower = less output\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.5, # Feel free to change this\n",
    "        \"do_sample\": True\n",
    "    }\n",
    "    output = pipe(messages, **generation_args)\n",
    "\n",
    "    # We only care about the generated text\n",
    "    return output[0][\"generated_text\"]\n",
    "\n",
    "prompt = \"What methods of payment does PetBacker accept?\"\n",
    "response = prompt_model(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating\n",
    "Now that we can generate the output from the RAG system we can evaluate the results. You can use the ROUGE score to evaluate the results. The ROUGE score is a metric that measures the similarity between two texts. You can find more information about the ROUGE score [here](https://en.wikipedia.org/wiki/ROUGE_(metric)).\n",
    "\n",
    "Be sure to take a look at the [documentation](https://github.com/google-research/google-research/tree/master/rouge) on how to calculate the ROUGE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores: {'rouge1': Score(precision=0.9615384615384616, recall=0.16891891891891891, fmeasure=0.28735632183908044), 'rougeL': Score(precision=0.5769230769230769, recall=0.10135135135135136, fmeasure=0.1724137931034483)}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Set the most fitting response as the reference\n",
    "# The reference is the correct answer to the prompt\n",
    "reference = \"\"\"\n",
    "The methods of payment vary by region. Some of the payment methods PetBacker accepts are direct bank in, and major credit cards, debit cards via Paypal.\n",
    "\"\"\"\n",
    "\n",
    "# Evaluation with ROUGE\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "# Use the previous response or generate a new one\n",
    "scores = scorer.score(response, reference)\n",
    "print(\"ROUGE scores:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Use different metrics to evaluate the results\n",
    "A few common options are:\n",
    "- BLEU\n",
    "- METEOR\n",
    "- Embedding-Based Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: install the libraries to implement the other metrics\n",
    "# TODO: Implement the other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Use a better (quantized) model\n",
    "We are using a relatively small model for the task above. If you are unhappy with the results and want better performance, consider using a larger model like [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct).\n",
    "\n",
    "We will also use quantization to lower GPU usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12c38ec3abb42d58693f2d4592eb4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/8.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2a1f01b24b487e9422fec598abeeb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df249bfd66cd4f8296f0bdf9d9db6ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3747537544cb44abbe61cf8aa41aee2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fc32291a1f4afe860648b19648a492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f75ad9ad49a4f98a8e692432b08b91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/701 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b504ac61c086422caf1bdf57ce152153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/790 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3feab80b341d4c88817d73f9089dde92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/29.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2a96894e2441f1971dc035e3389844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ff5d57ca3646bb98ec3b229cb6180b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optional: Use a larger (quantized) model with bitsandbytes\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_id = \"ibm-granite/granite-3.1-8b-instruct\"\n",
    "\n",
    "# use quantization to lower GPU usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\",\n",
    "    quantization_config=bnb_config,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# TODO: Make a pipeline to use this model\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "def prompt_model(prompt: str, k: int = 2):\n",
    "    formatted_prompt = format_prompt(prompt, k)\n",
    "\n",
    "    # Use the system prompt and the formatted prompt to generate the response\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "    ]\n",
    "\n",
    "    # The parameters for the LLM\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,  # Lower = less output\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.5,  # Feel free to change this\n",
    "        \"do_sample\": True,\n",
    "    }\n",
    "    output = pipe(messages, **generation_args)\n",
    "\n",
    "    # We only care about the generated text\n",
    "    return output[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "prompt = \"What methods of payment does PetBacker accept?\"\n",
    "response = prompt_model(prompt)\n",
    "print(response)\n",
    "\n",
    "# TODO: Evaluate it against the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Make a web interface for the model using gradio\n",
    "import gradio as gr\n",
    "\n",
    "gr.Interface(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Harvest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
