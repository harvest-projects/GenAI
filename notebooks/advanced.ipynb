{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/blog/not-lain/rag-chatbot-using-llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets sentence-transformers faiss-cpu accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "ST = SentenceTransformer(\"all-MiniLM-L6-v2\", cache_folder=\"models\")\n",
    "ds = load_dataset(\"JLK-ptbk/faq\", cache_dir=\"datasets\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from typing import List\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def parse_data(data_str: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parses a string representation of a list into an actual list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Safely evaluate the string to a Python list\n",
    "        data_list = ast.literal_eval(data_str)\n",
    "        if isinstance(data_list, list):\n",
    "            return data_list\n",
    "        else:\n",
    "            return []\n",
    "    except (SyntaxError, ValueError):\n",
    "        # If parsing fails, attempt to extract strings using regex\n",
    "        return re.findall(r\"'(.*?)'\", data_str)\n",
    "\n",
    "\n",
    "def remove_html(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes HTML tags from a string.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "\n",
    "def clean_entry(data_str: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Cleans a single 'data' field entry.\n",
    "    \"\"\"\n",
    "    parsed_list = parse_data(data_str)\n",
    "    cleaned_list = []\n",
    "    for item in parsed_list:\n",
    "        # Remove HTML tags\n",
    "        clean_text = remove_html(item)\n",
    "        # Normalize whitespace\n",
    "        clean_text = \" \".join(clean_text.split())\n",
    "        # Filter out entries that are too short or incomplete\n",
    "        if len(clean_text) > 10 and not re.search(\n",
    "            r\"\\bStartin\\b\", clean_text, re.IGNORECASE\n",
    "        ):\n",
    "            cleaned_list.append(clean_text)\n",
    "    return cleaned_list\n",
    "\n",
    "\n",
    "# Apply the cleaning function\n",
    "def apply_cleaning(example) -> dict:\n",
    "    cleaned = clean_entry(example[\"data\"])\n",
    "    cleaned = \"Q: \" + \" A: \".join(cleaned)\n",
    "    return {\"text\": cleaned}\n",
    "\n",
    "\n",
    "cleaned_ds = ds.map(\n",
    "    apply_cleaning, remove_columns=[\"Unnamed: 0\", \"index\", \"data\"]\n",
    ")\n",
    "\n",
    "print(cleaned_ds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings to the dataset\n",
    "ds_with_embeddings = cleaned_ds.map(\n",
    "    lambda example: {\"embeddings\": ST.encode(example[\"text\"])},\n",
    ")\n",
    "\n",
    "# Add the index to the data\n",
    "ds_with_embeddings.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, k: int = 3):\n",
    "    \"\"\"a function that embeds a new query and returns the most probable results\"\"\"\n",
    "    embedded_query = ST.encode(query)  # embed new query\n",
    "    scores, retrieved_examples = (\n",
    "        ds_with_embeddings.get_nearest_examples(  # retrieve results\n",
    "            \"embeddings\",\n",
    "            embedded_query,  # compare our new embedded query with the dataset embeddings\n",
    "            k=k,  # get only top k results\n",
    "        )\n",
    "    )\n",
    "    return scores, retrieved_examples\n",
    "search(\"rebook\")[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
    "You are given the extracted parts of a long document and a question. Provide the answer to the question based on the document.\n",
    "If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolLM-135M-Instruct\", cache_dir=\"models\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolLM-135M-Instruct\", cache_dir=\"models\"\n",
    ").to(\"cuda\")\n",
    "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(prompt, retrieved_documents, k):\n",
    "    \"\"\"using the retrieved documents we will prompt the model to generate our responses\"\"\"\n",
    "    PROMPT = f\"Q:{prompt}\\nContext:\"\n",
    "    for idx in range(k):\n",
    "        PROMPT += f\"{retrieved_documents['text'][idx]}\\n\"\n",
    "    return PROMPT\n",
    "\n",
    "\n",
    "def generate(formatted_prompt):\n",
    "    formatted_prompt = formatted_prompt[:2000]  # to avoid GPU OOM\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # More tokens = longer responses\n",
    "    outputs = model.generate(\n",
    "        inputs, max_new_tokens=200, temperature=0.5, top_p=0.9, do_sample=True, eos_token_id=terminators\n",
    "    )\n",
    "    response = outputs[0][inputs.shape[-1] :]\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "def rag_chatbot(prompt: str, k: int = 2):\n",
    "    scores, retrieved_documents = search(prompt, k)\n",
    "    formatted_prompt = format_prompt(prompt, retrieved_documents, k)\n",
    "    return generate(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG pipeline\n",
    "query = \"What methods of payment does PetBacker accept?\"\n",
    "\n",
    "response = rag_chatbot(query, k=2)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Evaluate the RAG Model\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Reference response for evaluation\n",
    "reference = \"\"\"\n",
    "The methods of payment vary by region. Some of the payment methods PetBacker accepts are direct bank in, and major credit cards, debit cards via Paypal.\n",
    "\"\"\"\n",
    "\n",
    "# Evaluation with ROUGE\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
    "scores = scorer.score(response, reference)\n",
    "print(\"ROUGE scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Harvest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
