{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets faiss-cpu sentence-transformers gradio rouge-score ipywidgets datasets beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import faiss\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Check if GPU is available\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\", cache_folder=\"models\")\n",
    "\n",
    "# Load generative model and tokenizer\n",
    "# or t5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", cache_dir=\"models\")\n",
    "generative_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\", cache_dir=\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from: https://huggingface.co/datasets/JLK-ptbk/faq\n",
    "faq_dataset = load_dataset(\"JLK-ptbk/faq\", cache_dir=\"datasets\", split=\"train\")\n",
    "\n",
    "# Inspect the first few examples\n",
    "for i in range(3):\n",
    "    print(faq_dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def parse_data(data_str: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parses a string representation of a list into an actual list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Safely evaluate the string to a Python list\n",
    "        data_list = ast.literal_eval(data_str)\n",
    "        if isinstance(data_list, list):\n",
    "            return data_list\n",
    "        else:\n",
    "            return []\n",
    "    except (SyntaxError, ValueError):\n",
    "        # If parsing fails, attempt to extract strings using regex\n",
    "        return re.findall(r\"'(.*?)'\", data_str)\n",
    "\n",
    "def remove_html(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes HTML tags from a string.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "def clean_entry(data_str: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Cleans a single 'data' field entry.\n",
    "    \"\"\"\n",
    "    parsed_list = parse_data(data_str)\n",
    "    cleaned_list = []\n",
    "    for item in parsed_list:\n",
    "        # Remove HTML tags\n",
    "        clean_text = remove_html(item)\n",
    "        # Normalize whitespace\n",
    "        clean_text = ' '.join(clean_text.split())\n",
    "        # Filter out entries that are too short or incomplete\n",
    "        if len(clean_text) > 10 and not re.search(r'\\bStartin\\b', clean_text, re.IGNORECASE):\n",
    "            cleaned_list.append(clean_text)\n",
    "    return cleaned_list\n",
    "\n",
    "# Apply the cleaning function\n",
    "def apply_cleaning(example) -> dict:\n",
    "    cleaned = clean_entry(example['data'])\n",
    "    return {'faq': cleaned}\n",
    "\n",
    "cleaned_dataset = faq_dataset.map(apply_cleaning, remove_columns=['Unnamed: 0', 'index', 'data'])\n",
    "\n",
    "print(cleaned_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset into a list of dictionaries\n",
    "import pandas as pd\n",
    "faq_pairs = []\n",
    "faq_list = cleaned_dataset[\"faq\"]\n",
    "for i in range(0, len(faq_list), 2):\n",
    "    faq_pairs.append({\"question\": faq_list[i], \"answer\": faq_list[i + 1]})\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(faq_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the documents\n",
    "answers = dataset['answer']\n",
    "embeddings = embedding_model.encode(dataset[\"answer\"])\n",
    "\n",
    "# Determine the dimensionality of the embeddings\n",
    "d = embeddings.shape[1]\n",
    "\n",
    "# Initialize the FAISS index\n",
    "index = faiss.IndexFlatL2(d)  # Using L2 distance; consider IndexHNSWFlat or others for larger datasets\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"Number of vectors in the index: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generative model\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "\n",
    "system_prompt !   \"You are an AI assistant helping users with their queries about PetBacker services.\"\n",
    ")\n",
    "\n",
    "\n",
    "def rag_qa(question, top_k=1):\n",
    "    # Encode the question to find similar answers\n",
    "    question_embedding = embedding_model.encode([question], convert_to_tensor=False)\n",
    "    question_embedding = np.array(question_embedding).astype(\"float32\")\n",
    "\n",
    "    # Search for the top_k most similar answers\n",
    "    distances, indices = index.search(question_embedding, top_k)\n",
    "\n",
    "    # Retrieve the relevant contexts\n",
    "    retrieved_answers = [answers[idx] for idx in indices[0]]\n",
    "\n",
    "    # Prepare the input for the generator\n",
    "    # Incorporate the system prompt\n",
    "    print(retrieved_answers)\n",
    "    context = ' '.join(retrieved_answers[0])\n",
    "    input_text = f\"{system_prompt}\\nQuestion: {question}\\nContext: {context}\"\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer.encode(\n",
    "        input_text, return_tensors=\"pt\", truncation=True, max_length=512\n",
    "    )\n",
    "\n",
    "    # Generate the answer\n",
    "    outputs = model.generate(inputs, max_length=150, num_beams=5, early_stopping=True)\n",
    "\n",
    "    # Decode the generated answer\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example question\n",
    "user_question = \"What payment options can I use on PetBacker?\"\n",
    "\n",
    "# Get the answer from RAG\n",
    "generated_answer = rag_qa(user_question)\n",
    "\n",
    "print(\"Q:\", user_question)\n",
    "print(\"A:\", generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG pipeline function\n",
    "def rag_pipeline(query, retrieval_model, faiss_index, documents):\n",
    "    query_embedding = retrieval_model.encode([query])\n",
    "    _, retrieved_indices = faiss_index.search(query_embedding, k=3)\n",
    "    context = \" \".join([documents[i] for i in retrieved_indices[0]])\n",
    "\n",
    "    input_text = f\"Context: {context} Query: {query}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = generative_model.generate(**inputs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG pipeline\n",
    "query = \"What is Hugging Face?\"\n",
    "response = rag_pipeline(query, embedding_model, index, documents)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Evaluate the RAG Model\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Reference response for evaluation\n",
    "reference = \"Hugging Face is a company creating open-source libraries.\"\n",
    "\n",
    "# Evaluation with ROUGE\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(response, reference)\n",
    "print(\"ROUGE scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model for multi-modal retrieval\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Example: Image retrieval logic\n",
    "# Assume \"image_features\" and \"text_features\" are pre-computed\n",
    "text_query = \"open-source libraries\"\n",
    "text_features = clip_model.get_text_features(clip_processor(text=[text_query], return_tensors=\"pt\"))\n",
    "\n",
    "# Multi-modal response: Extend to use image features if applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def generate_response(query):\n",
    "    return rag_pipeline(query, model, tokenizer, embedding_model, index, documents)\n",
    "\n",
    "\n",
    "gr.Interface(fn=generate_response, inputs=\"text\", outputs=\"text\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Harvest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
