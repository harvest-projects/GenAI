{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "%pip install datasets sentence-transformers faiss-cpu accelerate rouge-score bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We will be using a random dataset I found online that contains FAQ about a service called PetBacker. If you are finished with this notebook and like to experiment some more try out some other (small) datasets. \n",
    "\n",
    "You can find other datasets [here](https://huggingface.co/datasets?size_categories=or:%28size_categories:n%3C1K,size_categories:1K%3Cn%3C10K%29&sort=trending).\n",
    "\n",
    "#### Note\n",
    "I like to use the `cache_dir` parameter so that models and datasets gets saved in the working directory. This is useful if you want to later delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "dataset = load_dataset(\"JLK-ptbk/faq\", cache_dir=\"datasets\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "We need 3 models to solve this problem:\n",
    "- One to generate embeddings of the input sequences (also know as the retriever)\n",
    "- A LLM to generate the output sequences\n",
    "- A tokenizer to convert the output sequences to the desired format\n",
    "\n",
    "You can find the other options for the LLM model [here](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?params=0%2C3&official=true) on the open LLM leaderboard.\n",
    "If you want to try out other embedding models have a look at [this leaderboard](https://huggingface.co/spaces/mteb/leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the models\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\", cache_folder=\"models\")\n",
    "\n",
    "# Load the LLM and its tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM-1.7B-Instruct\"\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=\"models\",\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True,\n",
    ")#.to(\"cuda\")       Uncomment the \"to\" part to ensure the device is set to GPU rather than CPU, provided you have CUDA drivers\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"models\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llm,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the dataset\n",
    "If you inspect the dataset you will see that it includes HTML and CSS tags. This is not useful for our RAG model, so we have to remove them.\n",
    "\n",
    "It also seems to be a Python list saved a string, that consist of 2 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Inspect the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "We want the dataset to contain data in this format for each row:\n",
    "\n",
    "`{'text': \"Q: What methods of payment does PetBacker accept? A: The methods of payment vary by region...\"}`\n",
    "\n",
    "You can choose to create a new column or adjust the existing column that contains the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Feel free to import any libary for this step\n",
    "from bs4 import BeautifulSoup\n",
    "import ast\n",
    "\n",
    "# Remove HTML tags\n",
    "def remove_html_tags(text: str) -> str:\n",
    "    # TODO: Remove HTML tags from the text and return the cleaned text\n",
    "    return \n",
    "\n",
    "def apply_cleaning(row) -> dict:\n",
    "    # Get the data from the row\n",
    "    data = row[\"data\"]\n",
    "\n",
    "    # TODO: Deal with list formatting of the data\n",
    "    data_list = ...\n",
    "\n",
    "    # Remove HTML tags\n",
    "    data_list = [remove_html_tags(item) for item in data_list]\n",
    "\n",
    "    # TODO: Normalize the whitespace\n",
    "\n",
    "    # TODO: Format the text to be like \"Q: ... A: ...\"\n",
    "    return {\"text\" : ...}\n",
    "\n",
    "# Delete all the unused columns (and the old data column)\n",
    "cleaned_dataset = dataset.map(apply_cleaning, remove_columns=[\"Unnamed: 0\", \"index\", \"data\"])\n",
    "\n",
    "# Inspect the cleaned dataset\n",
    "print(cleaned_dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the embeddings\n",
    "Now that the dataset is clean we will generate the embeddings using the `embedding_model`. We will be using SentenceTransformers for this task. You can find the documentation [here](https://sbert.net/docs/sentence_transformer/usage/usage.html).\n",
    "\n",
    "This step is used to convert the data to vectors, so we can quickly lookup the most similar vectors to the input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate the embeddings for the cleaned dataset\n",
    "ds_with_embeddings = cleaned_dataset.map(lambda x : {\"embeddings\": ...})\n",
    "\n",
    "# TODO: Set the \"embeddings\" column as FAISS index\n",
    "# HINT: Have a look at the documentation: https://huggingface.co/docs/datasets/en/faiss_es\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving similar embeddings\n",
    "The next step is to retrieve the most similar embeddings to the input prompt. This is done in the following steps:\n",
    "- Convert the text to an embedding\n",
    "- Retrieve the most similar embeddings\n",
    "- Return the most similar text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get the right document for the prompt\n",
    "def search(prompt: str, k: int = 3) -> tuple:\n",
    "    # TODO: Embed the query\n",
    "    embedded_prompt = ...\n",
    "\n",
    "    # TODO: Search the dataset for the nearest examples\n",
    "    # HINT: Look at the documentation from the cell above\n",
    "    scores, retrieved_examples = ...\n",
    "\n",
    "    # Return the retrieved examples\n",
    "    return retrieved_examples[\"text\"]\n",
    "\n",
    "# Check if the search function works\n",
    "search(\"rebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the prompt\n",
    "With the function above we can lookup relevant information for a given prompt. The next step is to format the prompt in a way that the LLM can generate the correct output. We want to format the given prompt in a way like this:\n",
    "\n",
    "`\"Question: {User prompt} Context: {Retrieved text}\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(prompt: str, k:int) -> str:\n",
    "    \"\"\"using the retrieved documents we will prompt the model to generate our responses\"\"\"\n",
    "    # TODO: Use the search function to get the retrieved documents\n",
    "    retrieved_docs = search(prompt, k)\n",
    "\n",
    "    # TODO: remove the part of the retrieved documents that is not the answer\n",
    "    # HINT: So we only want to return the part that is after \"A: \"\n",
    "\n",
    "    # TODO: Join the retrieved documents into a single string\n",
    "\n",
    "    # TODO: Add the retrieved documents to the prompt as context\n",
    "    # HINT: You can use the following format: \"Question: prompt \\n Context: retrieved docs\"\n",
    "    formatted_prompt = ...\n",
    "\n",
    "    return formatted_prompt\n",
    "\n",
    "print(format_prompt(\"rebook\", 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System prompt\n",
    "In the cell below you can specify your system prompt. This is the prompt that the LLM will use to generate the output. You can experiment with different prompts to see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a system prompt\n",
    "system_prompt = \"\"\"You are ...\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the model\n",
    "def prompt_model(prompt: str, k: int = 3):\n",
    "    formatted_prompt = format_prompt(prompt, k)\n",
    "\n",
    "    # Use the system prompt and the formatted prompt to generate the response\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "    ]\n",
    "\n",
    "    # The parameters for the LLM\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.5,  # Feel free to change this\n",
    "        \"do_sample\": True,\n",
    "    }\n",
    "    output = pipe(messages, **generation_args)\n",
    "\n",
    "    # We only care about the generated text\n",
    "    return output[0][\"generated_text\"]\n",
    "\n",
    "prompt = \"What methods of payment does PetBacker accept?\"\n",
    "response = prompt_model(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating\n",
    "Now that we can generate the output from the RAG system we can evaluate the results. You can use the ROUGE score to evaluate the results. The ROUGE score is a metric that measures the similarity between two texts. You can find more information about the ROUGE score [here](https://en.wikipedia.org/wiki/ROUGE_(metric)).\n",
    "\n",
    "Be sure to take a look at the [documentation](https://github.com/google-research/google-research/tree/master/rouge) on how to calculate the ROUGE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set the most fitting response as the reference\n",
    "# The reference is the correct answer to the prompt\n",
    "reference = ...\n",
    "\n",
    "# TODO: Generate response (or use the respnse of the previous cell)\n",
    "response = ...\n",
    "\n",
    "# TODO: Evaluation with ROUGE\n",
    "scorer = ...\n",
    "scores = ...\n",
    "print(\"ROUGE scores:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Use different metrics to evaluate the results\n",
    "A few common options are:\n",
    "- BLEU\n",
    "- METEOR\n",
    "- Embedding-Based Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: install the libraries to implement the other metrics\n",
    "# TODO: Implement the other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Use a better (quantized) model\n",
    "We are using a relatively small model for the task above. If you are unhappy with the results and want better performance, consider using a larger model like [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct).\n",
    "\n",
    "We will also use quantization to lower GPU usage. If you are doing this Masterclass without GPU availability, you need to remove quantization as a whole (and possibly question if it is a good idea to try this part of the Masterclass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Use a larger (quantized) model with bitsandbytes\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_id = \"ibm-granite/granite-3.1-8b-instruct\"\n",
    "\n",
    "# use quantization to lower GPU usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")#.to(\"cuda\")       Uncomment the \"to\" part to ensure the device is set to GPU rather than CPU, provided you have CUDA drivers\n",
    "\n",
    "# TODO: Make a pipeline to use this model\n",
    "\n",
    "# TODO: Evaluate it against the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Make a web interface for the model using gradio\n",
    "import gradio as gr\n",
    "\n",
    "gr.Interface(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Harvest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
